\chapter{Conclusions and Future Work}

\section{Conclusions}

This project aimed to contribute:
\begin{enumerate}[$\mathcal{C}$1.]
  \item The first articulation of the proposed architecture, including the context needed to understand it. It does, by way of this report, particular the second and third chapters.
  \item A Python implementation of the new architecture and algorithm. This was created in order to meet the last contribution. The code is publicly available on Github.
  \item A suite of graded evaluations/tests that explore how the architecture and source separation algorithm work in practice. The project explores how the architecture and algorithm work in practice. However more questions are raised as to why it seems to break down, performing less strikingly in the larger more challenging cases.
\end{enumerate}

The main takeaway from the project is that the ORBM works in smaller problems, and the implications and applications if it does work in larger cases would be huge, so there is a motivation to continue this work going forward.

\section{Future Work}

Frean and Marsland should pursure exactly why the ORBM is not performing better than the RBM on the MNIST dataset. I would suggest ensuring both models have a similar range of weights, to ensure one does not over power the other. Or another approach would be to revert back to using a single model, except the model would be trained on all MNIST digits (0-9). Then a composite input would be shown to this model in the ORBM architecture and the reconstructions should separate the sources of that input. This would alleviate issues of weights having different ranges.
