\chapter{Backgroud}

 As the ORBM builds on the previous work of Restricted Boltzmann Machines and Sigmoid Belief networks, the concepts and previous work in source separation, generative models as well as background on RBMs and Sigmoid Belief Networks need to be introduced.


\section{Generative Models and Source Separation}

Generative models are a powerful way to model data. The rational behind them being that we aim to learn a model that can both create the training data and represent it. Input data, say images, could be mapped from raw values into some higher level features. Hinton gave a compelling argument why higher level features are desirable in the context of generative models~\cite{hinton:32723:vv}. \begin{quote} Consider, for example, a set of images of a dog. Latent variables such as the position, size, shape and color of the dog are a good way of explaining the complicated, higher-order correlations between the individual pixel intensities, and some of these latent variables are very good predictors of the class label. In cases like this, it makes sense to start by using unsupervised learning to discover latent variables (i.e. features) that model the structure in the ensemble of training images.\end{quote}

The ORBM proposed in this project aims to represent data generated by two independently acting causes and does so by combining two existing generative models, the Restricted Boltzmann Machine, and the Sigmoid Belief Network.

\subsection{Terminology in Generative Models observable and hidden variables}

Generative models are comprised of variables, often referred to as units. Some of these variables are observed, that is their state is known. These are often referred to as the `visible` units and are used to represent the training data. For example in the image domain, the visible units correspond to the pixels of the image.

The variables that are not observed, are latent variables, often referred to as `hidden units` as they are not observed.

Connections between units are used to encode relationships between the variables, where the relationship may be causal, such as in a Sigmoid Belief network or an encoding/representation in the Restricted Boltzmann Machine.

Collections of units, are often referred to as `patterns` or `vectors` in that they are represented by a vector or pattern of bits. For instance in the context of an image, the visible pattern would be the pixels of the image ravelled into a one dimensional vector.


\subsection{PGMs as a tool reasoning about generative models}

Probalistic Graphical Models or PGMs for short, are an expressive way to represent a collection of related, stochastic variables. If the graph is directed then the edges represent causation, this is also referred to a Bayesian network. Conversely, if the graph was undirected then edges represent a dependancy or mapping.

Figure \ref{F:PGM-example} shows an abstract example of a directed PGM, where B is the underlying cause of A, we cannot observe B directly, instead it's state is represented as a `belief` or a probability of being in a given state. Throughout this report, RBMs, Sigmoid Belief Networks and the proposed ORBM will be shown in this format.

\begin{wrapfigure}{r}{0.2\textwidth}
\begin{center}
  \includegraphics[width = 0.2\textwidth]{Assets/PGM_Example_1.png}
\caption{An example PGM, showing an observed variable `A` and it's hidden cause `B`.}
\label{F:PGM-example}
\end{center}
\end{wrapfigure}

\section{Sampling and inverting the model}

Sampling is the process of drawing samples from a distribution. It is used when the distribution we want samples from is intractable to calculate analytically. This is required to train generative models, as often the gradient to be climbed/descended involves calculating a probability over all the units in the generative model.

Inverting a generative model can be referred to as inference, the process of reasoning about what we do not know, given that of which we do know. In generative models this is the `posterior`, the probability distribution of the hidden (latent) variables, given we know the state of the observable (visible) units.

The algorithm introduced and evaluated in this report aims to invert the ORBM, to generate hidden representations given an input.

\subsection{Gibbs sampling, a subset of Markov Chain Monte Carlo}

Gibbs sampling is a special case of Markov Chain Monte Carlo, a technique for drawing sampling from a complex distribution. Sampling from the probability mass (or `joint distribution`) of a generative model is a common use case for Gibbs sampling.

Gibbs sampling explores the desired probability distribution, taking samples of that distribution's state, allowing iterations of exploration between drawing of a sample to ensure that the samples are independent. The process of taking a step between states is referred to as a \texttt{Gibbs iteration}.

Gibbs sampling is used for performing inference in RBMs and as result also in the ORBM. The \texttt{mixing time}, that is how many Gibbs iterations are needed to reach a satisfactory sample is an important part issue in the ORBM, in that more than one may be required.

\subsubsection{Mixing Time}

MCMC methods aim to approximate a distribution, by exploring likely states. As we often start this process from a random state, it's important that enough Gibbs steps are taken before a sample is drawn. This is because the random state may not be close any part of the true distribution we want to sample from, so by running the chain for many iterations we increase the likihood of samples being from the desired distribution.

This process of waiting for enough steps to before drawing samples is referred to as the \texttt{Mixing Time}. When Hinton when proposed a fast training for algorithm for RBMs and DBNs ~\cite{Hinton:2006:FLA:1161603.1161605}, one Gibbs step was sufficient in practice for training and using an RBM. The ORBM is not so fortunate.


\subsection{Reconstructions, visualising what the model has learnt}

Generative Models can create an internal representation given an input. They can also generate a faux input given an internal representation. Performing one Gibbs iteration, that is sampling from the hidden units given an input $ P(\tilde{h}|\tilde{v}) $ and then taking the generated hidden state and generating a faux input. The model tries to reconstruct the input.

  \subsubsection{Ancestral Sampling or fanstasies of the model}
  \todo%
  In the same way that a generative model uses reconstructions to try and recreate the  supplied input based purely on how it's represented that input, performing many, many (greater than 100) Gibbs iterations with no input pattern clamped allows the reconstructions to explore the probability mass that the model has built up during training. Sampling from these wanderings creates what are refferred to as `fantasies` or `dreams`. These give a sense of what the model has learnt, and can act a smoke test for if the model has actually capture anything.
  (TODO-CITE-PAPER-WITH-MNIST-DREAM-EVALUATION, they were crappy).

  \section{An intractable model for causes}
    \subsection{Sigmoid Belief Networks}
    \todo%

    The ORBM relies on the Sigmoid Belief Network to capture the causation. The Sigmoid Belief Network is composed of units with weights and a sigmoid activation function, akin to that of a perceptron linear threshold unit/Perceptron. The probability of a node being `on` is found by taking the weighted sum of all input to that node and applying a Sigmoid function or another activation function that ensures a values between $0$ and $1$.

    Belief Networks appear to be an intuitive way to model data in machine learning, as rich dependancies often present in real data can be expressed in it's architecture. Nodes in the network represent binary variables which are dependent on ancestor nodes, the degree of which is encoded in a weight on a directed edge between them.

    Performing inference in a Sigmoid Belief network would allow source separation in that each hidden unit could represent a cause. Meaning if a causes state could be inferred from an input item, individual causes could be examined for an input. For example if the input was an n by n image, the Sigmoid Belief Net makes the assumption that each pixel has an independent cause.

    Despite the Sigmoid Belief Network being expressive and providing a succinct encoding of inter-variable dependancies, the expressiveness is too rich such that performing inference is intractable. There do exist algorithms for performing inference in Sigmoid Belief Networks. For instance, the Belief Propagation algorithm \todocite{The paper where BP/Sum Prod proposed} operates on this encoding, calculating the probabilities of a given network state (i.e. the state of all the variables).  Belief Propagation is intractable to use as the number of variables grow \todocite{ the paper explaining intractable for belief prop}.

    This intractability arises from the Sigmoid Nets richness and the `explaining away effect`. Inference is required for training generative models making Sigmoid Belief Networks impractical to train. \todocite{It has been done, link to paper where they do it.}

    \subsection{Explaining Away creates a trade off between richness and tractability}\label{SS:Explaining-Away}
    \todo%
    The power of the Belief Network is also it's weakness, a rich structure that models a system of interest inherently has dependancies. In its minimal case explaining away can be seen in a 3 node network popularised by \todocite{AI-A-MODERN-APPROACH-TODO. TODO-GRAPHIC} as shown in figure \ref{F:Explaining-Away}. Each of the nodes represents a binary state. For instance $Burglar = 1$ means that the person owning the \texttt{Alarm} has been burgled. Also note how the connections between the units have arrows, this is causal.

    \begin{figure}[h]
    \begin{center}
      \includegraphics[width = 0.6\textwidth]{Assets/Explaining_Away.png}
    \caption{The famous Burglar, Earthquake, Alarm network showing a minimal case of explaining away.}
    \label{F:Explaining-Away}
    \end{center}
    \end{figure}


    In the network shown in figure \ref{F:Explaining-Away}, knowledge of the \texttt{Alarm} creates a dependance between \texttt{Burglar} and \texttt{Earthqaukes}. For instance, say the Alarm has gone off and we know an earthquake has occurred, our belief in being burgled decreases. The dependance in belief networks means that sampling from the network requires a longer Markov Chain to mix, as changing the value of \texttt{Earthquake}, effects the value of \texttt{Burglar}. \todowording{In a network with many connected nodes the dependence introduced makes sampling take longer. In the context of images, where there may be upwards of 1000 observable values, all with different dependancies this is intractable.} Neal showed this by comparing the number of gibbs iterations required for small enough error rates in~\cite{neal1992:connectionist}.

  \subsection{Boltzmann Machines}

A Boltzmann machine \todocite{Cite the Harmonium, markov field and Hopfield for deterministic example...}  has qualities in common with Belief Networks. Both are generative models with their nodes having probabilities of being active based on neighboring nodes. Connections between nodes have associated weights as shown in figure \ref{F:Boltzmann-Machine}. These weights are symmetric.
\todowording{Unlike a Belief Network, a Boltzmann Machine is a undirected network that allows cycles and thus more complex data can be captured.}

  \todowording{Connections between nodes no longer encode causal information, instead a depedancy, the difference being that a connection encodes a relationship as they are not directed.}

  \begin{figure}[h]
  \begin{center}
    \includegraphics[width = 0.6\textwidth]{Assets/Boltzmann_Machine.png}
  \caption{A Boltzmann Machine, the blue shaded nodes representing the observed variables, and the non-shaded nodes the latent variables.}
  \label{F:Boltzmann-Machine}
  \end{center}
  \end{figure}

  Performing gibbs sampling appears trivial in a Boltzmann Machine, in that to find the probability of a given unit being active a weighted input to that node is passed through a sigmoid function. However, in practice the recurrent nature of Boltzmann Machines makes sampling intractable.

  \todocite{TODO-REFENCE-PAPER-OF-THIS The Boltzmann Machine was shown, given an unreasonable amount of time, to be able to perform better than the state of the art at the time.}


  \section{Restricted Boltzmann Machines: A Strong assumption}

  While Boltzmann Machines are impractical to train and sample from as networks grow in size \todocite{Need to cite this\ldots} their architecture can be altered to alieviate these shortcomings. The restriction, proposed by \todocite{Hinton, a proper cite} requires the network to be a two layer bipartite network, each layer corresponding to the observed (visible) and latent (hidden) units. Connections are forbidden between the layer of hidden units and the layer of visible units respectively. An example Restricted Boltzmann Machine architecture is shown in figure~\ref{F:Restricted-Boltzmann-Machine}. The collection of hidden units, forming a layer are referred to as the \texttt{hidden layer}. The collection of visible units are referred to as the \texttt{visible layer}.

  \begin{figure}[h]
  \begin{center}
    \includegraphics[width = 0.6\textwidth]{Assets/RBM_Example.png}
  \caption{An example Restricted Boltzmann Machine with four hidden units, and five visible units. Note that the edges between units are not directed - representing a dependency not a cause. }
  \label{F:Restricted-Boltzmann-Machine}
  \end{center}
  \end{figure}

 The effect of this restriction is inference becomes tractable, as the latent variables no longer become dependant given the observed variables. This is illustrated in figure~\ref{F:Restricted-Boltzmann-Machine} the hidden unit $h_1$ is not dependant on $h_2$ wether or not we know anything about the visible units. This is the opposite of a Sigmoid Belief Network where knowledge of the visible units makes the hidden units dependant. By removing the recurence present in Boltzmann Machines, it reduces the expresiveness of the RBM network but makes the RBM useable in practice. \todocite{The paper about Boltzmann Machines being really good when actually left to find solution.}


  \subsection{Energy, and the log likelihood of the joint}

  An RBM models the joint distribution of hidden and visible states.
  The RBM assigns to every configuration of $\tilde{h}$ and $\tilde{v}$ an \texttt{Energy}, where the lower the energy, the more likely the RBMs configuration is to \textit{fall} into that state. Hopfield, in the context of what is now called the Boltzmann Machine~\cite{Hopfield01041982}, presented this energy as defined by the function as

  $$ E(\tilde{v},\tilde{h}) = -\sum_{i \in visible}{W_{0i}v_i}   -\sum_{j \in hidden}{W_{0j}h_j}  -\sum_{i,j}{v_ih_jW_{ji}}  $$

  The probability of the RBM being in a given configuration is the joint probability of $\tilde{h}$ and $\tilde{v}$.


  $$ P(\tilde{h},\tilde{v}) = \frac{1}{Z} \prod_{j,i} e^{h_jW_{ji}v_i} $$
  If we move to log space that becomes
  $$ \log P(\tilde{h},\tilde{v}) = \frac{1}{Z} \log \sum_{j,i} h_j W_{ji} v_i $$
  $\frac{1}{Z} $ is the partition function, which normalises the probability of the joint. Calculating this would require summing over all possible configurations of $h$ and $v$, which is intractable for practical numbers of units. For instance a 28 by 28 image corresponds to 784 visible units, and for say 10 hidden units this would amount to $2^{784} * 2^{10} $ possible configurations. We opt to work in terms of $P^\star$ which is the non-normalised probability of the joint over $h \text{ and } v$.
  So we arrive at
  \begin{equation}\label{eq:LogPJoint}
     \log P^\star(h, v) = \sum_i \sum_j e^{h_j v_i W_{ji}}
  \end{equation}

  \subsection{Gibbs Sampling in RBMs}

    \todowording{To describe the ORBM and how to perform inference, traditional RBMs need to be thought of in a different yet equivalent way}.\todocite{This is similar to Hinton infinitely deep sigmoid belief networks.}

  For the following discussion, the hidden units are indexed by $j$ and the visible units are indexed by $i$.
  To perform inference, create reconstructions, and train RBMs, Gibbs sampling is used to draw samples from $P(h|v)$ (referred to as sampling from the posterior) and $P(v|h)$ respectively. It is introduced here to be reference later in Gibbs in an ORBM. In an RBM the process of Gibbs sampling is as follows:
  \begin{itemize}
    \item One must sample from $P(\tilde{h}|\tilde{v})$ giving a hidden state $\tilde{h'}$
    \item Using this hidden state, a visible state is then generated, $\tilde{v'}$, by sampling from $P(\tilde{v'}|\tilde{h'})$. This process of generating a hidden pattern, and subsequent visible pattern is referred to as a Gibbs step.
    \item This chain of Gibbs steps between sampling from $P(h|v)$ and $P(v|h)$ can then be repeated as desired, the longer the chain the closer the samples will be to the true joint distribution that the model has learnt. For training an RBM Hinton showed that 1 step is often enough in practice, as one step is enough to infer a direction to adjust the weights in.
  \end{itemize}
   This process can be calculated in one step for all units in the target layer as they are independant of one another. The process of updating the hidden, then visible layers forms what is referred to as the \texttt{Gibbs Chain}. \todocite{Feel like I could  cite this again in like a CD paper or something.} This process is visualised at a layer level in figure \ref{F:Gibbs_Chain}

  \begin{figure}[h]
    \begin{center}
      \includegraphics[width=0.8\textwidth]{Assets/RBM-Gibbs-Chain.png}
    \end{center}
    \caption{A figure illustrating a Gibbs chain where left to right indicates a Gibbs iteration. Note this is \emph{not} a PGM.}
    \label{F:Gibbs_Chain}
  \end{figure}

  \subsubsection{The Gibbs update}\label{S:Gibbs-Update}

  In a standard RBM, updating a hidden unit $h_j$ when performing Gibbs sampling is calculated by finding $ P(h_j = 1 | \tilde{v}) $ where $\tilde{v}$ is an input pattern. In the context of an image, $ \tilde{v} $ would be the pixel values where each pixel corresponds to a visible unit, $v_i$.
  The probability of a given hidden unit activating is: \todocite{Gibbs sampling would be good here}.
  \begin{equation}\label{eq:Hid-Gibbs-Update}
  P(h_j = 1 | \tilde{v}) = \sigma(\psi_j)
  \end{equation}
  Where $\psi_j$ is the weighted sum into the $jth$ hidden unit and $\sigma()$ is the Sigmoid function, or it also known as the Logistic function $\sigma(x)=1/(1+e^{-x})$. Figure \ref{F:PSI} illustrates $\psi_j$ for an example RBM.

  \begin{figure}[h]
  \begin{center}
    \includegraphics[width = 0.8\textwidth]{Assets/PSI_and_PHI.png}
  \caption{A diagram showing $\psi_j$, the weighted sum into the $jth$ hidden unit. Note that $W_{oj}$ is the hidden bias, represented as a unit that is always on with a weight into each hidden unit.}
  \label{F:PSI}
  \end{center}
  \end{figure}

  As the weights are symmetric, sampling from the visible layer, given a hidden state is similar. That is $P(v_i = 1 | \tilde{h})$, where $\tilde{h}$ is the entire hidden vector is given by:
  \begin{equation}\label{eq:Vis-Gibbs-Update}
   P(v_i = 1 | \tilde{h}) = \sigma(\phi_{i})
  \end{equation}
  Where $\phi_i$ is the weighted sum into the $ith$ visible unit, which is: $ \phi_i = \sum(W_{ji}h_{j}) + W_{0i} $. Both $\phi_j$ and $\psi_i$ can be expressed in alternative, but useful way:
  \begin{equation}
  \phi_j = \log P^\star(v,h | v_i = 1) - \log P^\star(v,h | v_i = 0)
  \end{equation}
  \begin{equation}\label{psi-gibbs-update-rbm}
  \psi_i = \log P^\star(h,v | h_j = 1) - \log P^\star(h,v | h_j = 0)
  \end{equation}




    % \subsubsection{Tractable Training - Contrastive Divergence}
    % Hinton TODO-CITE-CLASSIC-PAPER proposed Contrastive Divergence as a method for training RBMs efficiently. The algorithm leverages the now tractable wake phase because $P(h|v)$ is efficeint to compute. However the free or sleep phase required another restriction where the network is only left to its own dynamics can be limited to only one iteration and still perform well. TODO-CITE-CD-PAPER

    % This restriction allows an efficient calculation of the Wake Phase of generative model learning, as the $ P(h|v) $ can be calculated as a simple weighted sum passed through a sigmoid followed by a bernouli trial where the probability of being $1$ is equal to the result of sigmoid.



  % \subsection{Deep Learning}
  %
  %   \begin{itemize}
  %     \item Discuss deep learning as there are clear parallels to Deep Belief Networks and the new approach
  %     \item in paritcular how the deep networks have this process of freezing the weights and creating a sigmoid belief layer instead. There seem to be clear parallels between a deep network with one RBM to the ORBM.
  %   \end{itemize}
  %
  % \begin{itemize}
  %   \item Unrolling the gibbs chain and we are in effect training an infinite depth sigmoid beleif net (TODO-REFERENCE-HINTONS-PAPER-HERE)
  % \end{itemize}

    \subsection{Evaluating Restricted Boltzmann Machines}
    A challenge faced by this project and work with RBMs is that they are non-trivial to evaluate \todocite{Something please}. Being unsupervised black box, one cannot inspect the hidden units for an input and be sure that a good model has been learnt.

    We can examine the weights into a given hidden unit in the shape of the visible vector. For instance in the context of images, \texttt{Hinton Diagrams} allow visualisation of what a given hidden unit is `doing` by visualising the matrix. This was first used by Hinton in the context of Boltzmann Machines in \cite{Hinton:1986:LRB:104279.104291}. They also give insight into hidden unit utilisation.  Weights are often initialised to small random values, the Hinton diagram will have no visual structure to it. This is illustrated in figures \ref{F:Hinton-Good} and \ref{F:Hinton-Bad}. The former showing a hidden units weights where some structure has been learnt, and the latter showing the opposite.

    \begin{wrapfigure}{L}{0.45\textwidth}
    \begin{center}
      \includegraphics[width = 0.2\textwidth]{Assets/HINTON1.png}
    \caption{.}
    \label{F:Hinton-Good}
    \end{center}
    \end{wrapfigure}
    \begin{wrapfigure}{R}{0.45\textwidth}
    \begin{center}
      \includegraphics[width = 0.2\textwidth]{Assets/HINTON2.png}
    \caption{.}
    \label{F:Hinton-Bad}
    \end{center}
    \end{wrapfigure}

    Evaluating RBMs is also problem dependant, if split by dimenionality of the task we have some different approaches:
    \begin{description}
    \item[Small Cases] In trivial cases an RBM can inspected analytically. Reconstructions of the dataset should match the dataset with approximately the correct proportion. For instance training RBMs on two bit XOR should result in mostly [1,0] and [0,1] but not [1,1] and [0,0].
    If the task has a small enough dimentionality, like two bit XOR, then a learned RBM can be compared to one constructed by hand. This approach is used later when the ORBM tackles two bit XOR.

    \item[Large Cases] In non-trivial cases, with larger datasets, reconstructions can be compared to the training dataset,  but empirically detecting if a model is trained is difficult given the unsupervised, black box nature of RBMs.
    We can train a classifier on the RBMs hidden representation, with the idea being that better hidden features should result in a better way of distinguishing classes, and therefore a better classification accuracy.

  \end{description}
