\chapter{Evaluation}

\section{Two bit XOR: The minimal case}\label{S:XOR}

\subsubsection{Description}

The starting point for the evalatuion is examining the ORBM reconstructions in a two bit pattern, where two of the same model are combining to form the data. This model makes one bit on in a two bit pattern, i.e. $[1 , 0]$ or $[0 , 1]$. The training set is the two bit XOR truth table and it provides the minimal case to compare the full correction calculation, versus that of the proposed approximation (see Equation \ref{eq:Full-Corretion} and \ref{eq:Approx-Correction} respectively).

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{Assets/Two-Bit-RBM.png}
  \end{center}
  \caption{The two bit RBM for modelling a single bit on at a time in a two bit pattern. The diagonal weights are negative and the non-diagonal weights are positive.}
  \label{F:Two-Bit-RBM}
\end{figure}

\subsubsection{Architecture and Parameters}


Being a trivial dimensionality, the RBMs weights were constructed by hand, meaning that only the ORBMs inference algorithm was being evaluated and not the training of the RBMs plugged into it. This network is picture in Figure \ref{F:Two-Bit-RBM}.

\subsubsection{Method}

This RBM was checked to ensure that it behaved in practice by ensuring it's reconstructions matched the input for a large frequency of reconstructions.
\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{Assets/XOR-ORBM-Creation-Process.png}
  \end{center}
  \caption{The process used to create the ORBM for evalating composite XOR inputs.}
  \label{F:XOR-ORBM-Creation-Process-Diagram}
\end{figure}
In a similar way to the reconstructions, free-phase visible pattern samples can be taken from the model and evaluated. In the small dimensions the dreams of the RBM should match the training set. A bar graph of frequencies of dreams was created and the RBM behaves as expected, generating dream patterns that match the training set in the correct proportion. With a valid model established, the XOR RBM was duplicated and plugged into the ORBM structure as illustrated in Figure \ref{F:XOR-ORBM-Creation-Process-Diagram}.

The inference algorithm was run in the ORBM architecture for various visible inputs, giving two hidden vectors for the two representations $h^A$ and $h^B$. For each pair of hidden vectors, a reconstruction was created, the process illustrated in Figure
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{Assets/XOR-2-Bit-Process-Diagram.png}
  \end{center}
  \caption{The process of generating reconstruction is shown in this diagram.}

  \label{F:Two-Bit-ORBM-Process-Diagram}
\end{figure}~\ref{F:Two-Bit-ORBM-Process-Diagram}. This process was repeated in a similar way to how the reconstructions were evaluated in the lone RBM, counting the frequency each reconstruction occurred over $1500$ runs.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|}
\hline
Visible Input & \multicolumn{2}{l|}{Expected Reconstructions} \\ \hline
$[1 , 1 ]$    & $[1, 0]$              & $[0,1]$               \\ \hline
$[1, 0 ]$     & $[1,0]$               & $[1, 0]$              \\ \hline
$[0, 1]$      & $[0,1]$               & $[0,1]$               \\ \hline
\end{tabular}
\caption{A Table showing the expected reconstructions from performing ORBM inference with various input patterns. The left and right hand column of the Expected Reconstructions colummn indicate the reconstructions from the left and right RBMs in the ORBM.}
\label{my-label}
\end{table}

\subsubsection{XOR ORBM Analysis}

The results of this process are shown in figure \ref{F:Two-Bit-RBM-Inference-Results-1}. The ORBM is able to separate the causes with both types of correction, as the model is being duplicated, it produces $[1,0]$ and $[0,1]$ approximately half the time and symmetrically $[0,1]\text{ }[1,0]$ the other half of the time. These reconstructions were compared to one of the RBMs trained to recognised a single pattern being on in two bits. As expected a machine that has been trained to recognise one bit, has no concept of two bits being on and hence the reconstructions show no mechanism for separating the sources. This is illustrated in the bottom right of Figure \ref{F:Two-Bit-RBM-Inference-Results-1}.

The results of repeating this process with the input $[1,0]$ yielded successful results. We would hope that ORBM architecture could also handle inference with just a single subject. The results of this are shown in the top right and bottom left of Figure \ref{F:Two-Bit-RBM-Inference-Results-1}.

The ORBM was able to extract the target patterns $[1,0]$ and $[0,1]$ given the input $[1,1]$. This was the case for both the Approximated and Full corrections, which gave confidence that the more computationally efficient Approximated correction could be relied on going forward --- as for larger datasets it is a lot faster in practice. The generative model also copes with the subject overlapping, which in the two bit case arised when $[1,0]$ is composed with $[1,0]$. In both the Approximated and Full corrections the highest occurring reconstruction is the target $[1,0]$, however there appears to be a lot less confidence.


\begin{landscape}
\begin{figure}
  \begin{center}
    \includegraphics[height=0.9\textheight]{Assets/results/xor-results}
  \end{center}
  \caption{A figure showing the result of generating 1000 reconstructions with an RBM and ORBM on various inputs.}
  \label{F:Two-Bit-RBM-Inference-Results-1}
\end{figure}
\end{landscape}



\section{$Y$ bit pattern, $X$ neighbouring bits on} %%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Description}
A natural next step from a single bit on in a 2 bit pattern, is moving up to $X$  bits side by side on in a $Y$ bit pattern. In affect this is modelling a $X$ bit subject, in a $Y$ bit pattern.
For example if $Y = 4 $ and $ X = 2 $ then a valid inputs are the rows of the following table:
\begin{equation}\label{eq:Example-xy-dataset} dataset =
\begin{bmatrix}
  1 & 1 & 0 & 0 \\
  0 & 1 & 1 & 0 \\
  0 & 0 & 1 & 1
\end{bmatrix}
\end{equation}

This allows for some interesting cases, for instance using the same example as above of $Y = 4 $ and $ X = 2 $, we can examine how the ORBM handles separating interesting patterns. For instance where \emph{subjects} partially overlap such as $[1,1,1,0]$, which is a composition of $[1,1,0,0]$ and $[0,1,1,0]$. This evaluation will explore these interesting cases over a subset of the possible combinations of $Y$ and $X$, ensuring that the ORBM is able to reconstuct the combination of images correctly.


\subsubsection{Architecture and Parameters}
\begin{itemize}
  \item $|Y|$ Hidden units per RBM, any less and I was unable to train the RBM to make the target reconstructions or dreams.
  \item The RBM was trained with $1000$ epochs and a learning rate of $0.002$.
  \item A sample of 10,000 dreams were sampled from the RBMs, and then plotted on a bar graph. The frequency of produced dreams was ensured to be approximately equal and that the dreams should directly match the training set. This is feasible as the ground truth patterns are known.
\end{itemize}


\subsubsection{Method and Results}

For a subset of values of $y \in Y, x \in X | 2 < x < y < 10$ the following process was repeated:

\begin{enumerate}
  \item A single RBM was trained on all possible permutations of $X$ neighbouring bits being on in a $Y$ bit string (as seen in the Matrix~\ref{eq:Example-xy-dataset}).
  \item The trained RBM was duplicated and plugged into the ORBM architecture.
  \item For a subset of possible compositions of two patterns, generate reconstructions using the ORBM (and the approximate correction). Where the cases examined are interesting cases such as partial overlapping and completely separate subjects.
  \item Finally reconstructions are plotted on bar graphs ensuring they match the expected results.
\end{enumerate}

A subset of the larger (but still small) dimension task results are shown in Figure \ref{F:X-Bit-ORBM-Inference-Results-1}. These illustrate the more interesting cases and the ORBM correctly separates the sources in all of them. The top left graph in Figure \ref{F:X-Bit-ORBM-Inference-Results-1} extends the work in section \ref{S:XOR} by recognising a single bit in a 5 bit pattern and the ORBM is successful. The top right graph illustrates the case of a 1 bit overlap of data made up of 2 neighboring bits. The bottom left graph also exemplifies a 1 bit overlay but this time with a `wider` pattern of 3 bits. The bottom right graph shows the case where the two `subjects` are side by side, not overlapping. This is a disparity here that would likely be solved creating more reconstructions.

\begin{landscape}
\begin{figure}
  \begin{center}
    \includegraphics[height=0.9\textheight]{Assets/results/xy-bit-results}
  \end{center}
  \caption{A figure showing interesting cases of ORBM reconstructions in $Y$ bit pattern, $X$ neighbouring bits on.}

  \label{F:X-Bit-ORBM-Inference-Results-1}
\end{figure}
\end{landscape}

\section{2D Patterns in a small dimensional space}%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Description}
This task continues to work with a single RBM being duplicated, instead increasing the dimensionality from $~10$ to $25$ in the form of a 5 by 5 pixel image. Then another similar model is introduced so the ORBM has to separate a 2 by 2 square and various sized rectangles.

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{Assets/results/sq-dataset.png}
  \end{center}
  \caption{A figure illustrating the dataset used for this task. 2 by 2 pixel squares in a 5 by 5 pixel image.}
  \label{F:Sq-Dataset}
\end{figure}

\subsubsection{Architecture and Parameters}
\begin{itemize}
  \item Uses a 25 hidden unit RBM trained on a dataset of every permutation of a 2 by 2 square in a 5 by 5 pixel image. This dataset is illustrated in Figure \ref{F:Sq-Dataset}.
  \item The trained RBMs reconstructions and dreams were visually inspected, as well as Hinton diagrams to ensure an effective model.
  \item When performing inference in the ORBM, 500 Gibbs iterations were used. In practice this number is quite large however as the dimensions are small it is not computational intractable, and this gives more confidence that the ORBM is performing to the best of its ability.
\end{itemize}

\subsubsection{Method and Results}

The method followed a similar process as previous evaluations: Generate the training data and training and verfiying the RBM to make sure it performs well.
Next the whole dataset was composited with every other item in that dataset, and ORBM reconstructions were then generated (using 500 Gibbs iterations when calculating the correction), as were RBM reconstructions.
A subset of compelling results from this process were extracted and are shown in Figure \ref{F:sq-orbm-results}. I only show a single RBM reconstruction here as the same model is being used for both subjects.

\subsubsection{Square results}
\begin{description}
  \item[Column 1] Shows two squares sitting side by side. As expected the RBM, only modeling a single square reconstructs a square in the middle of the two. The ORBM is able to successfully extract and reconstruct the separate squares.
  \item[Columns 2 and 3] Here we see a single pixel overlap, the ORBM successfully reconstructing the ground truth. The RBM, despite having identical weights to the RBM that is plugged into the ORBM there is no mechanism to separate the sources. The ORBM does not perform perfectly in column 3, in that one if it's reconstructions is correct but the other is noiser than the RBM.
  \item[Columns 4, 5 and 6] Much like column 1 we have disjoint subjects, the ORBM successfully separates them, however the RBM performs much, much worse.
\end{description}

This process was repeated except a different RBM was introduced, one that represents a rectangle instead of square. Plugging the original square model and the new rectangle into the ORBM architecture. The results for this aspect are shown in \ref{F:rect-orbm-results}.

\subsubsection{Rectangle results}
\begin{description}
  \item[Columns 1 and 2] These show two instances of the ORBM being able to separate a square and rectangle of different sizes.
  \item[Columns 3] This shows a confusing case where the square is completely occluded by the the larger square. The ORBM reconstructions look valid in that it reconstructs the correct shapes, unlike the RBM, however the placing of the 2 by 2 square is in fact incorrect.
  \item[Columns 4, 5 and 6] Here we see partially overlapping shapes of various sizes. The ORBM generates less noisy reconstructions compared to the RBM.
\end{description}




\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=\textwidth]{Assets/results/sq-orbm-results.png}
  \end{center}
  \caption{Figure illustrating a subset of the the results from ORBM inference on 2 By 2 square images.}
  \label{F:sq-orbm-results}
\end{figure}

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=\textwidth]{Assets/results/rect-sq-orbm-results.png}
  \end{center}
  \caption{Figure illustrating a subset of the the results from ORBM inference on 2 by 2 squares combined with $x$ by $y$ pixel rectangles. Note that there is a different RBM (and corresponding ORBM) being used for different values of $x$ and $y$ in the figure.}
  \label{F:rect-orbm-results}
\end{figure}


  \section{MNIST Digits Mixing Time and Reconstructions}

  \subsubsection{Description}
  MNIST is a widely used dataset of handwritten digits (0 -- 9). This evaluation explored the non-trivial task of given two handwritten digits composited to form one image, how effectively can the ORBM separate the sources compared to the naive RBM?

  \subsubsection{Architecture and Parameters}

  \begin{itemize}
    \item MNIST Digit images are 28 by 28 pixel images, with pixel values between 0 -- 1.
    \item Ten RBMs each with 100 Hidden units trained on 500 examples of each digit respectively. Reconstructions and dreams of these RBMs were inspected by hand to ensure that they resembled the dataset.
  \end{itemize}

  \subsubsection{Method and Results}

  For every digit dataset (of size 500), each digit in each dataset was composed with every other digit in every other dataset. Given this set of composite datasets, the corresponding RBMs were plugged in the ORBM architecture and used to create reconstructions.

  As an MNIST image has 784 (28 by 28 pixels) features, more consideration is need for how many Gibbs iterations should be used to generate the hidden states ($h^A$ and $h^B$) given the input. By examining reconstructions at different points in the Gibbs chain we can get a sense of how quickly the chain is mixing. A subset of these chains are illustrated in Figure \ref{F:MNIST-Mixing-Time}.
  \begin{figure}[htb]
    \begin{center}
      \includegraphics[width=0.8\textwidth]{Assets/results/Mixing-Results.png}
    \end{center}
    \caption{A figure visualising how reconstructions change in the ORBM as Gibbs iterations are run for two compositions of digits three and two.}
    \label{F:MNIST-Mixing-Time}
  \end{figure}
  By visually inspecting the reconstructions and the associated hidden activations (the hidden activations are not pictured) at Gibbs iterations 1,2,5,10,50,100, and 500 it can be seen the Gibbs chain mixes very quickly. Viewing these reconstructions as animations it is easier to see how the reconstructions stabilise and this lead to the choice of using 100 Gibbs iterations, just to be ensure the chain is definitely mixed while not being such a long chain that it is intractable to run.

  This diagram also reveals an interesting case where the hidden activations have turned completely off. The reconstruction however appears grey with all values pixel being $0.5$. As mentioned in Section \ref{S:Biases}, the ORBM does not use a visible bias which would be responsible for making an all zero reconstruction given no activations in the hidden layer.


  Given the results of the exploration of the mixing time, 100 Gibbs iterations were used when generating the hidden states ($h^A$ and $h^B$) for a given input. The RBMs plugged into the ORBM were also used to create standard RBM reconstructions, to compare against the ORBM. Given the ORBMs reconstructions (two per image) and the RBM reconstructions (also two per image), two scores were applied to compare these reconstructions to the ground truth:
  \begin{description}
    \item[Cross Entropy] The cross entropy was calculated between the reconstruction and the ground truth.
    \item[Cosine Angle] The angle between the flattened reconstruction vectors was computed, then negated to give a `score`. The higher the score the smaller the angle between the reconstruction vector and the ground truth.
  \end{description}
  These scores can then be summed over each digit and over the entire dataset to find a single score for each digit composed with every other digit forming a 9 by 9 matrix. A cell of this matrix (say $j,i$) corresponds to the difference between the ORBM score and RBM scores for digits $j$ and $i$ composited together. This entire process was repeated 10 times gain certainty in the results --- given the stochasticity of the RBM and ORBM. This process is shown in Algorithm \ref{alg:scores}.

  \begin{algorithm}[!ht]
   \KwData{MNIST Digit datasets, each with 500 examples, 10 RBMs trained on MNIST data 0--9 respectively}
   \KwResult{Composite Datasets for every digit}

   \For {10 repetitions to gain confidence in results (via the ECS Grid)} {
     \For{All MNIST digits datasets}{
      \For{Every digit example in the MNIST digit}{
        composite the current digit dataset with every other dataset\;
        Generate reconstructions on that dataset using the ORBM, RBM\;
        Calculate the Cosine Angle score and Cross Entropy for both the RBM's and ORBM's reconstructions versus the ground truth\;
      }
      Sum the scores over every digit example\;
     }
     Tabulate the summed scores in a several matrices indexed by the digits being composited\;
   }
  \caption{The algorithm explaining how the scores matrices were computed.}\label{alg:scores}
  \end{algorithm}

  By finding the difference between the ORBMs reconstructions and the RBMs reconstruction we can create another matrix, in which a positive value represents a `win` for the ORBMs reconstructions and a negative value represents a `loss` (where the RBM outperformed the ORBM). These score difference matrices are seen in for the Cross Entropy and Cosine Angle scores in \ref{F:Cross-Entropy-MNIST} and \ref{F:Cosine-MNIST} respectively, where the colour encodes the score. For the Cosine score,  using the approximate correction, zero composed with every other digit yielded the best results for ORBM relative to the RBM, and nine the converse yielding particularly bad results (worse by a factor of 10 compared to zero). Given this, I plotted the RBMs Score versus the ORBM Score as a scatter plot, where each point corresponds to a score. This allows us to examine if the ORBM is performing worse as a whole, or if in some cases it is performing better than the RBM. These plots for zero and nine are shown in \ref{fig:mnist-worse-best-results}.

  Lastly, the top two scoring ORBM reconstructions are shown in Figure \ref{F:Best-Results-MNIST} for the composition of a four and a five image and a two and four image respectively. In the four composited with five input, the two closest five ORBM reconstruction compared to the ground truth is shown along with the reconstruction from the other model (RBM B) connected to the ORBM.
  In the two composited with the four image, the two highest scoring four ORBM reconstructions are shown a long with the corresponding RBM reconstructions.
  The opposite, worst scores are illustrated in \ref{F:Worst-Results-MNIST}

  I selected four composited with five and two composited with four as the four is a challenging digit in that there are strikingly different ways to draw a four. As a human looking at the composition inputs, it is not immediately clear what parts of the image are caused by the four, and which are caused by the five or two respectively.


  \begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Assets/Cross-Entropy-Score.png}
      \caption{The difference between the RBM and ORBM cross entropy score summed over every item in the composite dataset.}
      \label{F:Cross-Entropy-MNIST}
  \end{subfigure}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[t]{0.45\textwidth}
      \includegraphics[width=\textwidth]{Assets/Cosine-Score.png}
      \caption{The difference between the RBM and ORBM cosine score summed over every item in the composite dataset.}
      \label{F:Cosine-MNIST}
  \end{subfigure}
  \caption{Dataset-wide MNIST Score Results }\label{fig:mnist-dataset-wide-results}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \includegraphics[width=\textwidth]{Assets/b.png}
        \caption{The cosine score for all digits composited with a handwritten zero.  }
      \label{F:Cosine-0-x-scores}
    \end{subfigure}
    \begin{subfigure}[t]{1\textwidth}
        \includegraphics[width=\textwidth]{Assets/(9,X)-ReconstructionScores.png}
        \caption{The cosine score for all digits composited with a handwritten nine.  }
      \label{F:Cosine-9-x-scores}
    \end{subfigure}
    \caption{Cosine Score breakdown for the highest and lowest performing datasets, 0 and 9.}\label{fig:mnist-worse-best-results}
\end{figure}

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.9\textwidth]{Assets/results/orbm-Best-2-results.png}
  \end{center}
  \caption{The top two ORBM reconstruction results for two different compositions. The highest scoring fives form the basis for the left part of the diagram. The highest scoring fours form the basis for the right part of the diagram.}
  \label{F:Best-Results-MNIST}
\end{figure}

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.9\textwidth]{Assets/results/orbm-Worst-2-results.png}
  \end{center}
  \caption{The worst two ORBM reconstruction results for two different compositions. The highest scoring fives form the basis for the left part of the diagram. The highest scoring fours form the basis for the right part of the diagram.}
  \label{F:Worst-Results-MNIST}
\end{figure}


\subsection{MNIST Analysis}

It is clear that RBM has better scores than the ORBM when summed over the dataset, and surprising the Approximated correction results in better scores than the Full Correction calculation. For the dataset wide scores \ref{fig:mnist-dataset-wide-results} I would expect to see symmetry in that, for example four composited with five, should score the same as five composited with four.

\section{Evaluation Analysis}

It is clear that the ORBM outperforms the RBM in source separation in the smaller tasks, such as XOR. However, as the size of the problems increased I had less confidence until ultimately it performed worse by the scores defined in the MNIST digit dataset. This amounts to the inference algorithm performing worse in large dimensions, with harder tasks. It is not related to the mixing time as that was explored on the same task that the ORBM had the most trouble with. In fact in examining the mixing time it appears it takes very few Gibbs iterations to actually arrive and settle at the wrong answer.
Yet in examining the individual scores for the various compositions of the MNIST dataset, the ORBM is performing worse in the majority of cases with only a small subset of examples scoring better than the RBM acting alone. Looking at example reconstructions, I would not say that it is that clear cut that the ORBM is performing worse.

As possible explanation could be that by plugging separately trained RBMs, into the ORBM architecture means that there is no regularisation of weights between the two RBMs. The RBMs were trained independently until they produced accurate reconstructions and dreams that approximately matched items from the dataset, however the process was adhoc and not identical per model. The reason this could be an issue is that the correction adjusts the visible input to the other model by roughly the weight. If the range of weights between the models is drastically different then one model could ``overpower'' the other. This would explain the effect where the hidden representation generated is entirely zero (no activation) as the other RBM is ``taking responsibility'' for the entirety of the input.
