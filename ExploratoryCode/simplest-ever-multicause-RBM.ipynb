{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 visibles, 2 rbms, with 2 hiddens each, and the same weights!\n",
    "I'm making a super simple example network here, to reality-check our thinking and test the thing out in the smallest possible example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "from pylab import *\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "pats = np.array([[0,0],[0,1],[1,0],[1,1]]) # just handy\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Our convention: hiddens are rows, visibles are columns.\n",
    "   * So our weights are W[hid,vis].\n",
    "   * Our hidden states should always be column vectors. In figures they'll use an ORANGE colormap (with darker meaning ON)\n",
    "   * Our visible states should always be row vectors. Use a BLUE colormap (with darker meaning ON)\n",
    "   * psi always refers to hiddens, phi always to visibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is showing the action in hA when v is clamped on visible, \n",
    "# and hB is the pattern on the hidden units of the OTHER rbm.\n",
    "\n",
    "def do_figure(fig_name, w, v, hB, version='EXACT'):\n",
    "    hB = hB.reshape((2,1))\n",
    "    pats = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    hA_prob = np.zeros(pats.shape, dtype=float)\n",
    "    for row, hA in enumerate(pats):\n",
    "        hA_prob[row,:], tmp = update_hidden_layer(hA, hB, w, v, version)\n",
    "    #print(hA_prob)\n",
    "\n",
    "    subplot(2,5,9)\n",
    "    imshow(v, interpolation='nearest',cmap='PuBu', vmin=0, vmax=1)\n",
    "    title('visible')\n",
    "    ax = axis('off')\n",
    "    subplot(2,5,5)\n",
    "    imshow(hB.T, interpolation='nearest',cmap='Oranges', vmin=0, vmax=1)\n",
    "    title('rbmB')\n",
    "    ax = axis('off')\n",
    "    subplot(2,6,1)\n",
    "    imshow(pats, interpolation='nearest',cmap='Oranges', vmin=0, vmax=1)\n",
    "    title('rbmA')\n",
    "    ax = axis('off')\n",
    "    subplot(2,5,2)\n",
    "    imshow(hA_prob, interpolation='nearest',cmap='Oranges', vmin=0, vmax=1)\n",
    "    title('orbm Pr(A)')\n",
    "    ax = axis('off')\n",
    "    subplot(2,5,3)\n",
    "    psiA = np.dot(v, w)\n",
    "    imshow(sigmoid(psiA), interpolation='nearest',cmap='Oranges', vmin=0, vmax=1)\n",
    "    title('rbm Pr(A)')\n",
    "    ax = axis('off')\n",
    "    savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_reconstructions(fig_name, w, v, version='EXACT'):\n",
    "    num_egs = 17\n",
    "    #visible_pats = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "    eg_hA = np.zeros(shape=(num_egs,2))\n",
    "    eg_hB = np.zeros(shape=(num_egs,2))\n",
    "    eg_vA = np.zeros(shape=(num_egs,2))\n",
    "    eg_vB = np.zeros(shape=(num_egs,2))\n",
    "    #i=0\n",
    "    #for subplotrow, v in enumerate(visible_pats):\n",
    "    v = v.reshape((1,2))\n",
    "    for r in range(num_egs):\n",
    "        hA = rng.randint(0,2,(2,1))\n",
    "        hB = rng.randint(0,2,(2,1))\n",
    "        for t in range(50):\n",
    "            prob, hA = update_hidden_layer(hA, hB, w, v, version)\n",
    "            prob, hB = update_hidden_layer(hB, hA, w, v, version)\n",
    "        eg_hA[r,:] = hA.ravel()\n",
    "        eg_hB[r,:] = hB.ravel()\n",
    "        # hiddens to visibles\n",
    "        phiA, phiB = np.dot(w,hA), np.dot(w,hB)\n",
    "        vA_prob, vB_prob = sigmoid(phiA), sigmoid(phiB)\n",
    "        eg_vA[r,:] = (vA_prob > rng.random(size=vA_prob.shape)).reshape(1,2)\n",
    "        eg_vB[r,:] = (vB_prob > rng.random(size=vB_prob.shape)).reshape(1,2)\n",
    "        \n",
    "        \n",
    "    subplot(1,5,1)\n",
    "    imshow(eg_vA, interpolation='nearest',cmap='PuBu', vmin=0, vmax=1)\n",
    "    axis('off')\n",
    "    subplot(1,5,2)\n",
    "    imshow(eg_hA+.1, interpolation='nearest',cmap='Oranges', vmin=0, vmax=1)\n",
    "    axis('off')\n",
    "    draw()\n",
    "    subplot(1,5,3)\n",
    "    imshow(v.reshape((1,2)), interpolation='nearest',cmap='PuBu', vmin=0, vmax=1)\n",
    "    axis('off')\n",
    "    subplot(1,5,4)\n",
    "    imshow(eg_hB, interpolation='nearest',cmap='Oranges', vmin=0, vmax=1)\n",
    "    axis('off')\n",
    "    subplot(1,5,5)\n",
    "    imshow(eg_vB, interpolation='nearest',cmap='PuBu', vmin=0, vmax=1)\n",
    "    axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_hidden_layer(h, other_h, w, v, version='APPROX'):  # THIS IS THE APPROX.\n",
    "    hA = h.reshape((2,1))\n",
    "    hB = other_h.reshape((2,1))\n",
    "    phiA = np.dot(w,hA).T # phiA is a column vector\n",
    "    phiB = np.dot(w,hB).T\n",
    "    phiA0 = phiA - hA*w \n",
    "    # phiA0 vector (cols) is like phiA but when each hidden in turn (rows) is off\n",
    "    if version == 'APPROX':\n",
    "        # i.e. phi_alts should now be same shape as w in fact! :(\n",
    "        sigA_to_A  = sigmoid(phiA0 + .5*w)\n",
    "        sigAB_to_A  = sigmoid(phiA0 + .5*w + phiB) \n",
    "        effective_visA = v + sigA_to_A - sigAB_to_A\n",
    "        our_psiA = (effective_visA * w).sum(1)\n",
    "    elif version == 'EXACT':\n",
    "        C = np.log(sigmoid(phiA0))\n",
    "        C = C - np.log(sigmoid(phiA0 + w))\n",
    "        C = C + np.log(sigmoid(phiA0 + w + phiB))\n",
    "        C = C - np.log(sigmoid(phiA0 + phiB))\n",
    "        our_psiA = (v*w + C).sum(1)        \n",
    "\n",
    "    hA_prob = sigmoid(our_psiA)\n",
    "    hA = (hA_prob > rng.random(size=hA_prob.shape)).reshape(2,1)\n",
    "    return hA_prob, hA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will set weights such that patterns 01 and 10  on the visible units get piles of sand (are \"memories\"), and 00 and 11 aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. -3.]\n",
      " [-3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "w = 3.0 * np.array([[1,-1],[-1,1]])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explaining away, example 1\n",
    "We clamp a visible pattern that requires both the rbms to pitch in. The B network explains the right-most bit.\n",
    "\n",
    "Under regular rbm dynamics, the A network is ambivalent as to which hidden state to choose: all 4 are equally likely.\n",
    "\n",
    "But under orbm dynamics we would expect the A network to be asked to explain the left-most bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAADmCAYAAADSmO5mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkiRJkiRJkiRJkiRJUvn+Cz6QaNXTSHv0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4732408>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v  = np.array([[1,1]])\n",
    "hB = np.array([[0,1]])\n",
    "do_figure('the_approx_way', w, v, hB, version='APPROX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAADmCAYAAADSmO5mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkiRJkiRJkiRJkiRJkiRJkiSpfP8FQl1oXcGxCP4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4a06ea8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "do_figure('the_exact_way', w, v, hB, version='EXACT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: we show the rbmA state alongside the orbm's Pr(A) here because in theory the orbm probabilities do depend on the existing state. However it is extremely heartening that they don't strongly. Perhaps because this example is just so symmetrical._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAD7CAYAAADXc3dDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABXdJREFUeJzt3TFO40oAx2E7CUVEjRAnoKFAVFRIuQUNZ0lyljS5RSQq\nKkRBwwkQ2npFAYm33uXpaT3v7zh+fF8/nsnI+9NI2QxVBQAAAAAAAAAA8L9Td/Xg0fVt03bMdrMq\nmqueHnX2OZLm55PWe1JVVTW/fyqab3R6MYh94avm54/W78ri6qxoruXL5yDek+b9o/WejGd3RXPt\nHtat92RUNBMAvxFTgAAxBQgQU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AAMQUI6Ow3uYf+O9o+7N6e\ni36bv7y5LJpvKL+55quSexwWj69Fc9XHJ4N4Tw79vg8nU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AA\nMQUIEFOAADEFCBBTgAAxBQiY9L2A76T0whK+n9JLS+iPkylAgJgCBIgpQICYAgSIKUCAmAIEiClA\ngJgCBIgpQICYAgSIKUCAmAIEiClAQGe3Ro1nd63HbDerornq6bpo3FDM75+Kxi1PL8Ir4ZAtrs76\nXkKnSvpQ0qFSTqYAAWIKECCmAAFiChAgpgABYgoQIKYAAWIKECCmAAFiChAgpgABYgoQUHf14Ob9\no2k7pvRSgt3DurPPkbR7e269J1VVVcuby6L5li+fg9gXvpqfT1q/K4vH16K56uOTQbwnJU0pVU+P\nWu+JkylAgJgCBIgpQICYAgSIKUCAmAIEiClAgJgCBIgpQICYAgSIKUCAmAIETPpeAMDfKLkIabtZ\ndbCSf+ZkChAgpgABYgoQIKYAAWIKECCmAAFiChAgpgABYgoQIKYAAWIKECCmAAFiChAgpgABYgoQ\nIKYAAWIKECCmAAFiChAgpgABYgoQIKYAAWIKECCmAAFiChAw6XsBAH9ju1n1vYR/5WQKECCmAAFi\nChAgpgABYgoQ4Nt8/rPR9W3T9xr2ZfewrvteA4fJyRQgQEwBAsQUIEBMAQLEFCBATAECOvuvUePZ\nXesxpRcZ1NN10bh9W95cFo2b3z+VzXd6UTSO/i0eX9uPuTrrYCXDVtKhUk6mAAFiChAgpgABYgoQ\nIKYAAWIKECCmAAFiChAgpgABYgoQIKYAAWIKECCmAAFiChAgpgABYgoQIKYAAWIKECCmAAFiChAg\npgABnf11Ur6P3cO67nsN0DcnU4AAMQUIEFOAADEFCOjsi4Pm/aNpO2Y8uyuaayhfgOzenlvvSVVV\n1fLmsmi+5cvnIPaFr+bnk9bvyuLxtWiu+vhkEO/J6Pq29Z5sN6uiuerpUes9cTIFCBBTgAAxBQgQ\nU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AAMQUIEFOAADEFCBBT\ngAAxBQgQU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AAMQUIEFOA\nADEFCBBTgIC6qwc37x9NV8/+Uz096uxzJM3PJ3vbk6qqquXL5yD2ha+anz/29+/n+GQQ78no+nZv\ne7J7WLfeEydTgAAxBQgQU4AAMQUIEFOAADEFCBBTgAAxBQgQU4AAMQUIEFOAADEFCJh09eDx7K71\nmO1m1cFKhm9+/1Q0bnl6EV4Jh2xxddb3EjpV0oeSDpVyMgUIEFOAADEFCBBTgAAxBQgQU4AAMQUI\nEFOAADEFCBBTgAAxBQgQU4AAMQUI6OzWqBL7vOEFIMnJFCBATAECxBQgQEwBAsQUIEBMAQLEFCBA\nTAECxBQgQEwBAsQUIEBMAQIO6qKT7WZVNK6ersMrAWjHyRQgQEwBAsQUIEBMAQLEFCBATAECxBQg\nQEwBAsQUIEBMAQLEFCBATAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgqH4ByNWbe1Vv15YA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4f07268>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_reconstructions('reconstructions', w, v=np.array([[1,1]]), version='EXACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-10-368cdcbbdf0f>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-368cdcbbdf0f>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def logL_orbm(w, train, num_Gibbs_iters=10):\n",
    "    # estimate the logL from samples.\n",
    "\n",
    "\n",
    "    # get sample proportions\n",
    "    \n",
    "    # KL divergence \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def samples_from_orbm_METHOD1(w, num_samples=1000, num_Gibbs_iters=10):\n",
    "    # make shitloads of samples in the \"naive\" way, using two entirely separate RBMs, which combine at the last moment.\n",
    "    samples = np.zeros((num_samples, 2), dtype = int)\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        uA = (0.5 > rng.random(size=(1,2))).reshape(1,2)\n",
    "        uB = (0.5 > rng.random(size=(1,2))).reshape(1,2)\n",
    "        for i in range(num_Gibbs_iters):\n",
    "            # set the hidden units in each network, which is a regular RBM.\n",
    "            psiA, psiB = np.dot(uA,w), np.dot(uB,w)\n",
    "            hA_prob, hB_prob = sigmoid(psiA), sigmoid(psiB)\n",
    "            hA = (hA_prob > rng.random(size=hA_prob.shape)).reshape(2,1)\n",
    "            hB = (hB_prob > rng.random(size=hB_prob.shape)).reshape(2,1)\n",
    "            # set the \"latent\" (u) units in the two regular RBMs\n",
    "            phiA, phiB = np.dot(w,hA).T,  np.dot(w,hB).T\n",
    "            uA_prob, uB_prob = sigmoid(phiA), sigmoid(phiB)\n",
    "            uA = (uA_prob > rng.random(size=uA_prob.shape)).reshape(1,2)\n",
    "            uB = (uB_prob > rng.random(size=uB_prob.shape)).reshape(1,2)\n",
    "        # finally, combine them to make a visible pattern: our sample\n",
    "        phiA, phiB = np.dot(w,hA).T,  np.dot(w,hB).T\n",
    "        vis_prob = sigmoid(phiA + phiB).reshape(1,2)\n",
    "        v = (vis_prob > rng.random(size=vis_prob.shape))\n",
    "        samples[j,:] = v\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def samples_from_orbm_METHOD2(w, num_samples=1000, num_Gibbs_iters=10):\n",
    "    # make shitloads of samples, by sampling using our scheme...\n",
    "    samples = np.zeros((num_samples, 2), dtype = int)\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        v = (0.5 > rng.random(size=(1,2))).reshape(1,2)  # initial vis pattern\n",
    "        hA = (0.5 > rng.random(size=(1,2))).reshape(1,2)  # initial vis pattern\n",
    "        hB = (0.5 > rng.random(size=(1,2))).reshape(1,2)  # initial vis pattern\n",
    "        for i in range(num_Gibbs_iters):\n",
    "            # reset the hidden units in the hidden layers, each of which is an ORBM.\n",
    "            tmp, hA = update_hidden_layer(hA, hB, w, v, 'EXACT')\n",
    "            tmp, hB = update_hidden_layer(hB, hA, w, v, 'EXACT')\n",
    "            # reset the visible units.\n",
    "            phiA, phiB = np.dot(w,hA).T,  np.dot(w,hB).T\n",
    "            vis_prob = sigmoid(phiA + phiB).reshape(1,2)\n",
    "            v = (vis_prob > rng.random(size=vis_prob.shape))\n",
    "        samples[j,:] = v\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 106\n",
      "(0, 1) 409\n",
      "(1, 0) 360\n",
      "(1, 1) 125\n"
     ]
    }
   ],
   "source": [
    "samples = samples_from_orbm_METHOD1(w, num_samples=1000, num_Gibbs_iters=500)\n",
    "new_array = [tuple(row) for row in samples]\n",
    "for y in [tuple(pat) for pat in pats]:\n",
    "    result= [x for x in new_array if x == y]\n",
    "    print(y, len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 87\n",
      "(0, 1) 400\n",
      "(1, 0) 432\n",
      "(1, 1) 81\n"
     ]
    }
   ],
   "source": [
    "samples = samples_from_orbm_METHOD2(w, num_samples=1000, num_Gibbs_iters=500)\n",
    "new_array = [tuple(row) for row in samples]\n",
    "for y in [tuple(pat) for pat in pats]:\n",
    "    result= [x for x in new_array if x == y]\n",
    "    print(y, len(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with weights of 1 that was pretty affirming: (i) the two methods agree, and (ii) the result seems sensible.\n",
    "\n",
    "But with larger weights (*5) there's a difference - why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from scratch\n",
    "\n",
    "Fuckity fuck: something's wrong here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_orbm(w, train, num_train_iters=10, num_Gibbs_iters=10):\n",
    "    \n",
    "    eta, momentum = .01, 0.25 # learning parameters\n",
    "    prev_dw = np.zeros(shape=w.shape)\n",
    "    for step in range(num_train_iters):\n",
    "        dw = np.zeros(shape=w.shape)\n",
    "        # Wake phase\n",
    "        for v in train: # 'clamped'\n",
    "            v = v.reshape(1,2) \n",
    "            #print('shape of v should be 1,2: ',v.shape)\n",
    "            # SAMPLE from hA and hB\n",
    "            # Start by initialising as if it were a vanilla RBM \n",
    "            psiA, psiB = np.dot(v,w), np.dot(v,w)\n",
    "            hA_prob, hB_prob = sigmoid(psiA), sigmoid(psiB)\n",
    "            hA = (hA_prob > rng.random(size=hA_prob.shape)).reshape(2,1)\n",
    "            hB = (hB_prob > rng.random(size=hB_prob.shape)).reshape(2,1)\n",
    "            #print('shape of hA should be 2,1: ',hA.shape)\n",
    "            for t in range(num_Gibbs_iters):\n",
    "                # update hA\n",
    "                tmp, hA = update_hidden_layer(hA, hB, w, v, 'EXACT')\n",
    "                # update hB\n",
    "                tmp, hB = update_hidden_layer(hB, hA, w, v, 'EXACT')\n",
    "    \n",
    "            phiA, phiB = np.dot(w,hA).T,  np.dot(w,hB).T\n",
    "            phiA0 = phiA - hA*w \n",
    "            phiB0 = phiB - hB*w \n",
    "            sigAB_toA  = sigmoid(phiA0 + .5*w + phiB)\n",
    "            sigAB_toB  = sigmoid(phiB0 + .5*w + phiA)\n",
    "            dwA = sigmoid(phiA)*hA + (v - sigAB_toA)*hA\n",
    "            dwB = sigmoid(phiB)*hB + (v - sigAB_toB)*hB\n",
    "            #COMBINE\n",
    "            dw = dw + (dwA + dwB)\n",
    "    \n",
    "        # Sleep phase\n",
    "        for v in train: # 'free'\n",
    "            vA = rng.randint(0,2,(1,2))\n",
    "            vB = rng.randint(0,2,(1,2))\n",
    "            for t in range(num_iters):\n",
    "                # visibles to hiddens\n",
    "                psiA, psiB = np.dot(vA,w), np.dot(vB,w)\n",
    "                hA_prob, hB_prob = sigmoid(psiA), sigmoid(psiB)\n",
    "                hA = (hA_prob > rng.random(size=hA_prob.shape)).reshape(2,1)\n",
    "                hB = (hB_prob > rng.random(size=hB_prob.shape)).reshape(2,1)\n",
    "                # hiddens to visibles\n",
    "                phiA, phiB = np.dot(w,hA), np.dot(w,hB)\n",
    "                vA_prob, vB_prob = sigmoid(phiA), sigmoid(phiB)\n",
    "                vA = (vA_prob > rng.random(size=vA_prob.shape)).reshape(1,2)\n",
    "                vB = (vB_prob > rng.random(size=vB_prob.shape)).reshape(1,2)\n",
    "    \n",
    "            dwA = vA*hA  # simple Hebb\n",
    "            dwB = vB*hB  # simple Hebb\n",
    "            #COMBINE\n",
    "            dw = dw - (dwA + dwB)\n",
    "    \n",
    "        w = w + eta * (dw  +  momentum * prev_dw)\n",
    "        prev_dw = dw\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = 1.0 * np.array([[1,-1],[-1,1]])\n",
    "train  = np.array([[0,1], [1,0]])# our data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old weights:\n",
      "[[ 1. -1.]\n",
      " [-1.  1.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_iters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-54950316b03f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_orbm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_Gibbs_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new weights:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-915b12dd8260>\u001b[0m in \u001b[0;36mlearn_orbm\u001b[1;34m(w, train, num_train_iters, num_Gibbs_iters)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mvA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mvB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[1;31m# visibles to hiddens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mpsiA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpsiB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_iters' is not defined"
     ]
    }
   ],
   "source": [
    "print('old weights:')\n",
    "print(w)\n",
    "\n",
    "w = learn_orbm(w, train, num_train_iters=100, num_Gibbs_iters=1) \n",
    "print('new weights:')\n",
    "print(w)\n",
    "\n",
    "v  = np.array([[1,1]])\n",
    "hB = np.array([[0,1]])\n",
    "do_figure('after_learning', w, v, hB, version='EXACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAD7CAYAAADXc3dDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABXhJREFUeJzt3T9KXWkAh+G5cxUkdQhkB0KakM5GcBkWriUna7FwGUIa\nu5Am4A4EsQ6CXs4sINX55r1/nHme/ne+Y/NyQPxczfM8/wXAv/L3vl8A4L9ATAECYgoQEFOAgJgC\nBMQUIHC0rQevzy4Xbza312OHnWztx0hNp8dDu6/ffw7tVh8+De04AL+fFk+mLx+HjpruX4Z2O/f8\nuniyvrgaOmpzd7N448sUICCmAAExBQiIKUBATAECYgoQEFOAgJgCBMQUICCmAAExBQistvZvSw78\n72j3YX78NbT7dv55aPdm/uaaP4zc4zD9eBg77N37sd2OHfp9H75MAQJiChAQU4CAmAIExBQgIKYA\nATEFCIgpQEBMAQJiChAQU4CAmAIExBQgIKYAATEFCIgpQEBMAQJiChAQU4CAmAIExBQgIKYAATEF\nCIgpQEBMAQJiChBYzfM8b+PB67PLxZvN7fXYYSdHY7sdm06Ph3Zfv/8c2q0+fBracQB+Py2eTF8+\nDh013b8M7Xbu+XXxZH1xNXTU5u5m8caXKUBATAECYgoQEFOAgJgCBMQUICCmAAExBQiIKUBATAEC\nYgoQEFOAwNYuOjn0Swn2YX78NbT7dv55aPdmLrDgDyOX4kw/HsYOe/d+bLdjh355ki9TgICYAgTE\nFCAgpgABMQUIiClAQEwBAmIKEBBTgICYAgTEFCAgpgABMQUIiClAQEwBAmIKEBBTgICYAgTEFCAg\npgABMQUIiClAQEwBAmIKEBBTgICYAgTEFCAgpgABMQUIiClAQEwBAmIKEBBTgICYAgTEFCAgpgAB\nMQUIiClAQEwBAmIKEBBTgICYAgTEFCAgpgCBo32/AG/f+uxy36+wM5u7m32/AgfKlylAQEwBAmIK\nEBBTgICYAgTEFCAgpgABMQUIiClAQEwBAmIKEBBTgICYAgTEFCAgpgABMQUIiClAQEwBAmIKEBBT\ngICYAgTEFCAgpgABMQUIiClAQEwBAmIKEBBTgMDRvl+At29zd7PvV4C982UKEBBTgICYAgTEFCCw\nmud53saD12eXizeb2+uxw07exu/RptPjod3X7z+HdqsPn4Z2HIDfT4sn05ePQ0dN9y9Du517fl08\nWV9cDR018ktVX6YAATEFCIgpQEBMAQJiChAQU4CAmAIExBQgIKYAATEFCIgpQEBMAQJiChAQU4CA\nmAIExBQgIKYAATEFCIgpQEBMAQJiChAQU4CAmAIExBQgIKYAATEFCIgpQEBMAQJiChAQU4CAmAIE\nxBQgIKYAATEFCIgpQEBMAQJiChAQU4DAap7neStPfn5dPFlfXA0dtbm7Gdrt2vz4a2j37fzz0G66\nfxnasX/T6fHyzY+HscPevR/b7dj67HLxZnN7PXbYydHiiS9TgICYAgTEFCAgpgABMQUIiClAQEwB\nAmIKEBBTgICYAgTEFCAgpgABMQUIiClAQEwBAmIKEBBTgICYAgTEFCAgpgABMQUIiClAQEwBAmIK\nEBBTgICYAgRW8zzP23jw+uxy8WZzez122MnR2G7HptPjod3X7z+HdqsPn4Z2HIDfT4sn05ePQ0dN\n9y9Du517fl08WV9cDR21ubtZvPFlChAQU4CAmAIExBQgIKYAATEFCIgpQEBMAQJiChAQU4CAmAIE\nxBQgsLWLTg79UoJ9mB9/De2+nX8e2r2ZCyz4w8ilONOPh7HD3r0f2+3YoV+e5MsUICCmAAExBQiI\nKUBATAECYgoQEFOAgJgCBMQUICCmAAExBQiIKUBgexedAPyP+DIFCIgpQEBMAQJiChAQU4CAmAIE\nxBQgIKYAATEFCIgpQEBMAQJiChAQU4CAmAIExBQgIKYAATEFCIgpQEBMAQJiChD4B4uarzv1UNsT\nAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_reconstructions('reconstructions', w*5, v=np.array([[1,1]]), version='EXACT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    " \n",
    " 1. two-bit learning: work in progress....\n",
    " \n",
    " 2. 3-visible-bit RBMs, two of them with shared weights.\n",
    " \n",
    " Something like\n",
    "  * 001 has logP=1\n",
    "  * 011 has logP=2\n",
    "  * 111 has logP=1\n",
    " \n",
    " 3. same but with two sets of weights, learning both of them.\n",
    " \n",
    "Where the complement is\n",
    "  * 110 has logP=1\n",
    "  * 100 has logP=2\n",
    "  * 000 has logP=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
