{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss for softmax \"neural\" classifiers\n",
    "Suppose we have data consisting of a set of $N$ patterns $\\mathbf{x}$ which are $D$ elements long, each with a target class $\\in \\{1..K\\}$. We can think of this as a $N\\times D$ matrix $X$, and a $N\\times K$ matrix $T$. each row of $T$ contains zeros except from a single 1 in the column corresponding to the correct target class.\n",
    "\n",
    "Consider a feed-forward neural network. When the network is given the input vector $ \\mathbf{x}_{n} $ it generates an output vector $\\mathbf{y}_{n}$ via the softmax function.\n",
    "We can make an $N\\times K$ matrix $Y$ from these output vectors (one per row), and thus the network maps $X \\rightarrow Y$ and $Y$ has the same dimensions as $T$. The softmax function is\n",
    "$$ Y_{n,i} = \\frac{\\exp(\\phi_{n,i})}{Z_n}  \n",
    "$$\n",
    "where $\\phi_{n,i} = \\sum_j W_{i,j} X_{n,j}$ and $Z_n = \\sum_k \\exp(\\phi_{n,k})$.\n",
    "\n",
    "In lecture we met the \"cross entropy\" loss function:\n",
    "$$ E = \\sum_n \\sum_k T_{n,k} \\log Y_{n,k} $$\n",
    "\n",
    "(Note that sometimes it's more convenient to write this as\n",
    "$$ E = \\sum_n \\log Y_{n, c_n} $$\n",
    "where $ c_n $ is the _index of the target class_ for the $ n^\\text{th} $ item in the training set.)\n",
    "\n",
    "We motivated the cross-entropy loss by arguing that it is the _log likelihood_, ie. the probability that a stochastic form of this network would generate precisely the training classes, namely to use the softmax outputs as a _categorical distribution_ and sample classes from it.\n",
    "\n",
    "(See  [this](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/)\n",
    "which discusses cross-entropy _vs_ training error _vs_ sum-of-squared errors without refering to a stochastic model).\n",
    "\n",
    "Q: Consider a simple neural network with no hidden layers and a softmax as the output layer. Show mathematically that gradient descent of the cross entropy loss in leads to the \"delta rule\" for the weight change: $\\Delta W_{ij} \\propto \\sum_n (T_{n,i} - Y_{n,i}) X_{n,j}$\n",
    "\n",
    "*Slightly easier option* : do it for the 2-class case. Hint: since there are only two options (say $a$ and $b$) and we know the second probability has to be $Y_b = 1-Y_a$, it's enough to worry about $Y_a$ alone, and this means we don't need two neurons computing two different $\\phi$ values. Instead, just find one, $Y_i = Prob(class=a)$, and this can be implemented by a _sigmoid_ (or _logistic_) non-linearity applied to $\\phi$.\n",
    "In this case the log likelihood is a sum over all items in the training set of this:\n",
    "$$ T_{n,i} \\log Y_{n,i} + (1-T_{n,i}) \\log (1- Y_{n,i})$$\n",
    "which you can differentiate and reorganise to get the answer : the delta rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
