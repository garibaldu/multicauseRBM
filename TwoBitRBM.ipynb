{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from rbm import RBM\n",
    "from sampler import VanillaSampler, PartitionedSampler\n",
    "from trainer import VanillaTrainier\n",
    "from performance import Result\n",
    "import numpy as np\n",
    "import datasets, performance, plotter, mnist, pickle, rbm, os, logging, sampler\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to logging.DEBUG \n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make one 2 bit image, both visibles on\n",
    "v = np.ones(2)\n",
    "\n",
    "# lets make our hiddens, 2 hidden units\n",
    "h_a = np.zeros(2)\n",
    "h_b = np.zeros(2)\n",
    "h_a[0] = 1\n",
    "h_a[1] = 0\n",
    "\n",
    "h_b[0] = 0\n",
    "h_a[1] = 1\n",
    "\n",
    "# Set up our weights matrix (|h| , |v|), with perfect weights.\n",
    "w_a = np.ones((2, 2))\n",
    "w_b = np.ones((2, 2))\n",
    "w_a[0][1] = -1\n",
    "w_a[1][0] = -1\n",
    "w_b[0][1] = -1\n",
    "w_b[1][0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 1.  1.]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = RBM(2,2,1)\n",
    "B = RBM(2,2,1)\n",
    "# manually set our RBM's\n",
    "A.hidden = h_a\n",
    "B.hidden = h_b\n",
    "A.visible = v\n",
    "B.visible = v\n",
    "A.weights = w_a\n",
    "B.weights = w_b\n",
    "A.hidden_bias = 0.5\n",
    "B.hidden_bias = 0.5\n",
    "A.visible_bias = 0.5\n",
    "B.visible_bias = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_matrix_for_runs(runs, model, visible):\n",
    "    van_sampler = VanillaSampler(model)\n",
    "    counts = {}\n",
    "    for i in range(runs):\n",
    "\n",
    "        vis = np.where(van_sampler.hidden_to_visible(van_sampler.visible_to_hidden(visible)) >= 0.5, 1, 0)\n",
    "        key = np.array_str(vis)\n",
    "        if key in counts:\n",
    "            counts[key] = counts[key] + 1\n",
    "        else:\n",
    "            counts[key] = 0\n",
    "    for key in counts:\n",
    "        counts[key] = counts[key]/runs * 100\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0 1]': 23.53, '[1 0]': 22.79, '[1 1]': 53.65}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run vanilla sampling 10,000 times, getting the percentage of reconstructions for A and then B\n",
    "print(count_matrix_for_runs(10000, A, v))\n",
    "print(count_matrix_for_runs(10000, B, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_sum_into_vis(vis_idx, w,h):\n",
    "    return np.dot(w.T, h)\n",
    "\n",
    "def visible_to_hidden(visible):\n",
    "    np.dot(w.T,visibe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __bernouli_flip__(weighted_sum):\n",
    "        p = expit(weighted_sum) > np.random.rand(*weighted_sum.shape)\n",
    "        return np.where(p, 1, 0)\n",
    "    \n",
    "def calc_correction(hidden_a, hidden_b, weights_a, weights_b):        \n",
    "    phi_a = np.dot(hidden_a, weights_a)\n",
    "    phi_b = np.dot(hidden_b, weights_b)\n",
    "\n",
    "    on_weights_a = (hidden_a * weights_a)\n",
    "    off_weights_a = (1 - hidden_a) * weights_a\n",
    "\n",
    "    on_weights_b =  (hidden_b * weights_b)\n",
    "    off_weights_b = (1 - hidden_b) * weights_b\n",
    "    \n",
    "    j_off_a = phi_a - on_weights_a\n",
    "    j_off_b = phi_b - on_weights_b\n",
    "    j_on_a = phi_a + off_weights_a\n",
    "    j_on_b = phi_b + off_weights_b\n",
    "    \n",
    "    correction_a = np.log(expit(j_off_a))  - np.log(expit(j_off_a + phi_b)) + np.log(expit(j_on_a + phi_b)) - np.log(expit(j_on_a))\n",
    "    correction_b = np.log(expit(j_off_b))  - np.log(expit(j_off_b + phi_a)) + np.log(expit(j_on_b + phi_a)) - np.log(expit(j_on_b))\n",
    "    \n",
    "    return correction_a, correction_b\n",
    "\n",
    "\n",
    "\n",
    "def run_partitioned_samples(hidden_a, hidden_b, weights_a, weights_b, visible,num_samples, vis_bias_a, vis_bias_b, hid_bias_a, hid_bias_b):\n",
    "    for epoch in range(num_samples):\n",
    "\n",
    "        phi_a = np.dot(hidden_a, weights_a) + vis_bias_a\n",
    "        phi_b = np.dot(hidden_b, weights_b) + vis_bias_b\n",
    "        \n",
    "        correction_a, correction_b = calc_correction(hidden_a, hidden_b, weights_a, weights_b)\n",
    "        \"\"\"\n",
    "        Apply the correction to the weighted sum into the hiddens\n",
    "        \"\"\"\n",
    "        psi_a = np.dot(visible ,weights_a.T) + correction_a.sum() + hid_bias_a\n",
    "        psi_b = np.dot(visible ,weights_b.T) + correction_b.sum() + hid_bias_b\n",
    "\n",
    "        # now, do we turn on he hiddens? Bernoulli sample to decide\n",
    "        hidden_a = __bernouli_flip__(psi_a)\n",
    "        hidden_b = __bernouli_flip__(psi_b)\n",
    "\n",
    "    return hidden_a, hidden_b\n",
    "\n",
    "def runit(times, sample_times,h_a, h_b, w_a, w_b,v):\n",
    "    wins = []\n",
    "    for i in range(times):\n",
    "        h_a, h_b = run_partitioned_samples(h_a,h_b, w_a,w_b, v, sample_times, 0.5, 0.5,0.5,0.5)\n",
    "        wins.append((h_a != h_b).all())\n",
    "        \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins:39 losses:161\n"
     ]
    }
   ],
   "source": [
    "# do 200 runs of partitioned sampling with each run finding the hidden state with 10,000 gibbs alternations\n",
    "wins = runit(200,10000, h_a, h_b, w_a,w_b,v)\n",
    "win_count = np.array(wins).sum()\n",
    "lose_count = len(wins) - win_count\n",
    "print(\"wins:{} losses:{}\".format(win_count, lose_count))\n",
    "# damn we wanted to wins to be greater than the losses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
